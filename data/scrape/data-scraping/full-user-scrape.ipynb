{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\":7,\"status_message\":\"Invalid API key: You must be granted a valid key.\",\"success\":false}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.themoviedb.org/3/authentication\"\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer 8d19786a6f3393e55afc3d8b7b85663f\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\":7,\"status_message\":\"Invalid API key: You must be granted a valid key.\",\"success\":false}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.themoviedb.org/3/discover/movie?include_adult=false&include_video=false&language=en-US&page=1&sort_by=popularity.desc\"\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer 8d19786a6f3393e55afc3d8b7b85663f\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\":7,\"status_message\":\"Invalid API key: You must be granted a valid key.\",\"success\":false}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://api.themoviedb.org/3\"\n",
    "api_key=\"8d19786a6f3393e55afc3d8b7b85663f\"\n",
    "api_str = f\"api_key={api_key}\"\n",
    "\n",
    "url = f'{base_url}/discover/movie?include_adult=false&include_video=false&language=en-US&page=1?{api_str}'\n",
    "response = requests.get(url)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "username = \"neahlekan\"\n",
    "\n",
    "base_url = f\"https://letterboxd.com/{username}/films/page/\"\n",
    "first_page_url = f\"{base_url}1/\"\n",
    "response = requests.get(first_page_url)\n",
    "if response.status_code != 200:\n",
    "    print(\"Failed to retrieve the webpage\")\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting additional page URLs\n",
    "pages = [first_page_url]\n",
    "pagination = soup.find('div', class_='paginate-pages')\n",
    "if pagination:\n",
    "    page_index = int(pagination.find_all('a')[-1].text)\n",
    "page_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting film data\n",
    "films = {}\n",
    "for page in range(1, page_index):\n",
    "    url = f\"{base_url}{page}/\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage: {page}\")\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    for li in soup.select('ul.poster-list li.poster-container'):\n",
    "        film_slug = li.find('div', class_='film-poster')['data-film-slug']\n",
    "\n",
    "        metadata = li.find('p', class_='poster-viewingdata')\n",
    "        rating = metadata.find('span', class_='rating')\n",
    "        if rating:\n",
    "            rating = rating.text\n",
    "            # ★★★★★ = 10\n",
    "            # ★★★★½ = 9.5\n",
    "            rating = rating.count('★') + rating.count('½') * 0.5\n",
    "        else:\n",
    "            rating = None\n",
    "        \n",
    "        like = metadata.find('span', class_='like')\n",
    "        if like:\n",
    "            like = True\n",
    "        else:\n",
    "            like = False\n",
    "        \n",
    "        review = metadata.find('a', class_='review-micro')\n",
    "        if review:\n",
    "            review = True\n",
    "        else:\n",
    "            review = False\n",
    "        \n",
    "        films[film_slug] = {\n",
    "            'rating': rating,\n",
    "            'like': like,\n",
    "            'review': review\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for film_slug, value in films.items():\n",
    "    base_url = f\"https://letterboxd.com/{username}/film/{film_slug}\"\n",
    "\n",
    "    if value['review']:\n",
    "        url = base_url + \"/reviews\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve the webpage: {film_slug}\")\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        table = soup.find('section', class_='viewings-list').find('ul')\n",
    "        \n",
    "        # Initialize a list to store extracted data\n",
    "        table_data = []\n",
    "        \n",
    "        # Iterate through each list item in the table\n",
    "        for li in table.find_all('li', class_='film-detail'):        \n",
    "            # Extract rating\n",
    "            rating = li.find('span', class_='rating').text.strip()\n",
    "            rating = rating.count('★') + rating.count('½') * 0.5\n",
    "\n",
    "            \n",
    "            # Extract date\n",
    "            date = li.find('span', class_='date').find('span').text.strip()\n",
    "            try:\n",
    "                date = datetime.datetime.strptime(date, '%d %b %Y').strftime('%Y-%m-%d')\n",
    "            except:\n",
    "                try:\n",
    "                    date = li.find('span', class_='date').find('time')['datetime']\n",
    "                    date = datetime.datetime.strptime(date, '%Y-%m-%dT%H:%M:%S.%fZ').strftime('%Y-%m-%d')\n",
    "                except:\n",
    "                    print(f\"Failed to parse date: {date} for film {film_slug}\")\n",
    "            \n",
    "            # Extract review content\n",
    "            review_content = li.find('div', class_='body-text').text.strip()\n",
    "                    \n",
    "            # Append extracted data to the list\n",
    "            table_data.append({\n",
    "                'rating': rating,\n",
    "                'date': date,\n",
    "                'review_content': review_content,\n",
    "            })\n",
    "    else:\n",
    "        url = base_url + \"/activity\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve the webpage: {film_slug}\")\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        table_data = []\n",
    "\n",
    "        # Find the table element\n",
    "        table = soup.find('div', class_='activity-table')\n",
    "        if table:\n",
    "            # Find all sections containing activity rows\n",
    "            activity_rows = table.find_all('section', class_='activity-row -basic')\n",
    "\n",
    "            # Extract data from each activity row\n",
    "            for row in activity_rows:\n",
    "                data = {}\n",
    "\n",
    "                activity_description = row.find('p', class_='activity-summary')\n",
    "                if activity_description:\n",
    "                    # Type\n",
    "                    text = activity_description.text\n",
    "                    data['activity_type'] = []\n",
    "                    if \"liked\" in text:\n",
    "                        data['activity_type'].append(\"like\")\n",
    "                    if \"rated\" in text:\n",
    "                        data['activity_type'].append(\"rating\")\n",
    "                    if \"reviewed\" in text:\n",
    "                        data['activity_type'].append(\"review\")\n",
    "\n",
    "                    # Link\n",
    "                    link = activity_description.find('a', class_='target')['href']\n",
    "                    data['activity_link'] = link\n",
    "\n",
    "                    # Actions on other users\n",
    "                    data['social'] = False\n",
    "                    if username not in link:\n",
    "                        data['social'] = True\n",
    "\n",
    "                time_tag = row.find('time')\n",
    "                if time_tag:\n",
    "                    # '2024-03-08T09:03:24.911Z'}\n",
    "                    date = datetime.datetime.strptime(time_tag['datetime'], '%Y-%m-%dT%H:%M:%S.%fZ').strftime('%Y-%m-%d')\n",
    "                    data['date'] = date\n",
    "\n",
    "                table_data.append(data)\n",
    "\n",
    "    min_date = min([row['date'] for row in table_data])\n",
    "    value['date'] = min_date\n",
    "    # aggregating reviews\n",
    "    if value['review']:\n",
    "        value[\"review_content\"] = \"\"\n",
    "        for row in table_data:\n",
    "            value[\"review_content\"] += row['review_content'] + \"\\n\\n\"\n",
    "\n",
    "        # remove terminal newlines\n",
    "        value[\"review_content\"] = value[\"review_content\"].strip()\n",
    "\n",
    "    value['activity'] = table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('films.json', 'w') as f:\n",
    "    json.dump(films, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
