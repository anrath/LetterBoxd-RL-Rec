{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "random.seed(8888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movie_data = pd.read_csv('../data/movie_data.csv')\n",
    "movie_data.dropna(subset=['year_released'], inplace=True)\n",
    "drop_cols = ['image_url', 'imdb_id', 'imdb_link', 'tmdb_id', 'tmdb_link']\n",
    "movie_data['year_released'] = movie_data['year_released'].astype('Int16')\n",
    "movie_data['runtime'] = movie_data['runtime'].astype('Int16', errors='ignore')\n",
    "movie_data.set_index('movie_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>genres</th>\n",
       "      <th>image_url</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>imdb_link</th>\n",
       "      <th>movie_title</th>\n",
       "      <th>original_language</th>\n",
       "      <th>overview</th>\n",
       "      <th>popularity</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>tmdb_id</th>\n",
       "      <th>tmdb_link</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>year_released</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>football-freaks</th>\n",
       "      <td>5fc85f606758f69634496fd3</td>\n",
       "      <td>[\"Music\",\"Animation\"]</td>\n",
       "      <td>film-poster/4/6/4/4/4/0/464440-football-freaks...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Football Freaks</td>\n",
       "      <td>en</td>\n",
       "      <td>Football crazy, football mad. Don’t watch this...</td>\n",
       "      <td>0.600</td>\n",
       "      <td>[\"United Kingdom\"]</td>\n",
       "      <td>1971-12-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>535272.0</td>\n",
       "      <td>https://www.themoviedb.org/movie/535272/</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aftermath-1960</th>\n",
       "      <td>5fc85ff26758f696344ace0c</td>\n",
       "      <td>[]</td>\n",
       "      <td>film-poster/2/4/5/5/0/0/245500-aftermath-0-230...</td>\n",
       "      <td>tt0586129</td>\n",
       "      <td>http://www.imdb.com/title/tt0586129/maindetails</td>\n",
       "      <td>Aftermath</td>\n",
       "      <td>en</td>\n",
       "      <td>Aftermath was the pilot for an unsold TV serie...</td>\n",
       "      <td>0.600</td>\n",
       "      <td>[]</td>\n",
       "      <td>1960-04-17</td>\n",
       "      <td>22.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>318331.0</td>\n",
       "      <td>https://www.themoviedb.org/movie/318331/</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>where-chimneys-are-seen</th>\n",
       "      <td>5fc85f606758f69634496fcd</td>\n",
       "      <td>[\"Drama\"]</td>\n",
       "      <td>film-poster/9/3/3/1/8/93318-where-chimneys-are...</td>\n",
       "      <td>tt0045731</td>\n",
       "      <td>http://www.imdb.com/title/tt0045731/maindetails</td>\n",
       "      <td>Where Chimneys Are Seen</td>\n",
       "      <td>ja</td>\n",
       "      <td>Gosho’s most celebrated film both in Japan and...</td>\n",
       "      <td>1.568</td>\n",
       "      <td>[\"Japan\"]</td>\n",
       "      <td>1953-03-05</td>\n",
       "      <td>108.0</td>\n",
       "      <td>[\"日本語\"]</td>\n",
       "      <td>117779.0</td>\n",
       "      <td>https://www.themoviedb.org/movie/117779/</td>\n",
       "      <td>6.6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the-musicians-daughter</th>\n",
       "      <td>5fc85f606758f69634496fd1</td>\n",
       "      <td>[\"Drama\"]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tt0187327</td>\n",
       "      <td>http://www.imdb.com/title/tt0187327/maindetails</td>\n",
       "      <td>The Musician's Daughter</td>\n",
       "      <td>en</td>\n",
       "      <td>Carl Wagner's good wife was dying. His heart b...</td>\n",
       "      <td>0.600</td>\n",
       "      <td>[\"United States of America\"]</td>\n",
       "      <td>1911-12-12</td>\n",
       "      <td>15.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>560377.0</td>\n",
       "      <td>https://www.themoviedb.org/movie/560377/</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50-years-of-fabulous</th>\n",
       "      <td>5fc85f606758f69634496fd4</td>\n",
       "      <td>[\"Documentary\"]</td>\n",
       "      <td>film-poster/4/5/4/6/0/3/454603-50-years-of-fab...</td>\n",
       "      <td>tt4769914</td>\n",
       "      <td>http://www.imdb.com/title/tt4769914/maindetails</td>\n",
       "      <td>50 Years of Fabulous</td>\n",
       "      <td>en</td>\n",
       "      <td>50 Years of Fabulous recounts the rich history...</td>\n",
       "      <td>0.600</td>\n",
       "      <td>[]</td>\n",
       "      <td>2018-05-17</td>\n",
       "      <td>75.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>525187.0</td>\n",
       "      <td>https://www.themoviedb.org/movie/525187/</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              _id                 genres  \\\n",
       "movie_id                                                                   \n",
       "football-freaks          5fc85f606758f69634496fd3  [\"Music\",\"Animation\"]   \n",
       "aftermath-1960           5fc85ff26758f696344ace0c                     []   \n",
       "where-chimneys-are-seen  5fc85f606758f69634496fcd              [\"Drama\"]   \n",
       "the-musicians-daughter   5fc85f606758f69634496fd1              [\"Drama\"]   \n",
       "50-years-of-fabulous     5fc85f606758f69634496fd4        [\"Documentary\"]   \n",
       "\n",
       "                                                                 image_url  \\\n",
       "movie_id                                                                     \n",
       "football-freaks          film-poster/4/6/4/4/4/0/464440-football-freaks...   \n",
       "aftermath-1960           film-poster/2/4/5/5/0/0/245500-aftermath-0-230...   \n",
       "where-chimneys-are-seen  film-poster/9/3/3/1/8/93318-where-chimneys-are...   \n",
       "the-musicians-daughter                                                 NaN   \n",
       "50-years-of-fabulous     film-poster/4/5/4/6/0/3/454603-50-years-of-fab...   \n",
       "\n",
       "                           imdb_id  \\\n",
       "movie_id                             \n",
       "football-freaks                NaN   \n",
       "aftermath-1960           tt0586129   \n",
       "where-chimneys-are-seen  tt0045731   \n",
       "the-musicians-daughter   tt0187327   \n",
       "50-years-of-fabulous     tt4769914   \n",
       "\n",
       "                                                               imdb_link  \\\n",
       "movie_id                                                                   \n",
       "football-freaks                                                      NaN   \n",
       "aftermath-1960           http://www.imdb.com/title/tt0586129/maindetails   \n",
       "where-chimneys-are-seen  http://www.imdb.com/title/tt0045731/maindetails   \n",
       "the-musicians-daughter   http://www.imdb.com/title/tt0187327/maindetails   \n",
       "50-years-of-fabulous     http://www.imdb.com/title/tt4769914/maindetails   \n",
       "\n",
       "                                     movie_title original_language  \\\n",
       "movie_id                                                             \n",
       "football-freaks                  Football Freaks                en   \n",
       "aftermath-1960                         Aftermath                en   \n",
       "where-chimneys-are-seen  Where Chimneys Are Seen                ja   \n",
       "the-musicians-daughter   The Musician's Daughter                en   \n",
       "50-years-of-fabulous        50 Years of Fabulous                en   \n",
       "\n",
       "                                                                  overview  \\\n",
       "movie_id                                                                     \n",
       "football-freaks          Football crazy, football mad. Don’t watch this...   \n",
       "aftermath-1960           Aftermath was the pilot for an unsold TV serie...   \n",
       "where-chimneys-are-seen  Gosho’s most celebrated film both in Japan and...   \n",
       "the-musicians-daughter   Carl Wagner's good wife was dying. His heart b...   \n",
       "50-years-of-fabulous     50 Years of Fabulous recounts the rich history...   \n",
       "\n",
       "                         popularity          production_countries  \\\n",
       "movie_id                                                            \n",
       "football-freaks               0.600            [\"United Kingdom\"]   \n",
       "aftermath-1960                0.600                            []   \n",
       "where-chimneys-are-seen       1.568                     [\"Japan\"]   \n",
       "the-musicians-daughter        0.600  [\"United States of America\"]   \n",
       "50-years-of-fabulous          0.600                            []   \n",
       "\n",
       "                        release_date  runtime spoken_languages   tmdb_id  \\\n",
       "movie_id                                                                   \n",
       "football-freaks           1971-12-05      0.0               []  535272.0   \n",
       "aftermath-1960            1960-04-17     22.0               []  318331.0   \n",
       "where-chimneys-are-seen   1953-03-05    108.0          [\"日本語\"]  117779.0   \n",
       "the-musicians-daughter    1911-12-12     15.0               []  560377.0   \n",
       "50-years-of-fabulous      2018-05-17     75.0               []  525187.0   \n",
       "\n",
       "                                                        tmdb_link  \\\n",
       "movie_id                                                            \n",
       "football-freaks          https://www.themoviedb.org/movie/535272/   \n",
       "aftermath-1960           https://www.themoviedb.org/movie/318331/   \n",
       "where-chimneys-are-seen  https://www.themoviedb.org/movie/117779/   \n",
       "the-musicians-daughter   https://www.themoviedb.org/movie/560377/   \n",
       "50-years-of-fabulous     https://www.themoviedb.org/movie/525187/   \n",
       "\n",
       "                         vote_average  vote_count  year_released  \n",
       "movie_id                                                          \n",
       "football-freaks                   0.0         0.0           1971  \n",
       "aftermath-1960                    8.0         1.0           1960  \n",
       "where-chimneys-are-seen           6.6        10.0           1953  \n",
       "the-musicians-daughter            0.0         0.0           1911  \n",
       "50-years-of-fabulous              0.0         0.0           2018  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_movie_data(movie_data):\n",
    "    \"\"\"\n",
    "    Converts a dictionary of movie data into a string format suitable for LLM fine-tuning.\n",
    "    \n",
    "    Args:\n",
    "        movie_data (dict): A dictionary containing movie attributes such as title, language,\n",
    "                           overview, and other metadata.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted string representation of the movie data.\n",
    "    \"\"\"\n",
    "    # Define the order and the keys to include in the string\n",
    "    keys_order = [\n",
    "        \"movie_title\", \"rating_val\", \"genres\", \"year_released\", \"popularity\", \"vote_average\", \"vote_count\", \"runtime\", \"production_countries\", \"original_language\", \"spoken_languages\", \"overview\"\n",
    "    ]\n",
    "    \n",
    "    # Building the string with key-value pairs\n",
    "    formatted_string = \" | \".join(f\"{key}: {movie_data.get(key, 'N/A')}\" for key in keys_order if key in movie_data)\n",
    "    \n",
    "    return formatted_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nan2list = lambda x: x if type(x) is str else '[]'\n",
    "\n",
    "def format_movie_data_v2(movie_data): # , user_rating, user_review):\n",
    "    release_yr = movie_data.get(\"year_released\", None)\n",
    "    title_fmt = movie_data.get(\"movie_title\", \"N/A\") + (f\" ({release_yr})\" if release_yr else \"\")\n",
    "    genre_fmt = ' and '.join([x.lower() for x in json.loads(nan2list(movie_data.get('genres', \"[]\")))])\n",
    "    runtime = movie_data.get('runtime', None)\n",
    "    if runtime is not None and not np.isnan(runtime):\n",
    "        hours = int(runtime // 60)\n",
    "        minutes = int(runtime % 60)\n",
    "        runtime_fmt = f\"{hours}h {minutes}m\"\n",
    "    else:\n",
    "        runtime_fmt = \"N/A\"\n",
    "    \n",
    "    avg_rating = movie_data.get('vote_average')\n",
    "    votes = movie_data.get('vote_count')\n",
    "    if votes is not None and not np.isnan(votes) and votes > 0:\n",
    "        votes = int(votes)\n",
    "        avg_rating_fmt = f\"{avg_rating:.2f} ({votes} vote(s))\"\n",
    "    else:\n",
    "        avg_rating_fmt = \"N/A\"\n",
    "    \n",
    "    production_countries_fmt = ' and '.join(json.loads(nan2list(movie_data.get('production_countries', \"[]\")))) or 'N/A'\n",
    "    languages_fmt = ' and '.join(json.loads(nan2list(movie_data.get('spoken_languages', \"[]\")))) or 'N/A'\n",
    "    overview = movie_data.get(\"overview\", \"N/A\")\n",
    "    \n",
    "    return f\"\"\"\n",
    "Title: {title_fmt}\n",
    "Genres: {genre_fmt}\n",
    "Runtime: {runtime_fmt}\n",
    "Average rating: {avg_rating_fmt}\n",
    "Production countries: {production_countries_fmt}\n",
    "Languages: {languages_fmt}\n",
    "Overview: {overview}\n",
    "\"\"\".strip()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stringification V1\n",
      "movie_title: Pelican Blood | genres: [\"Drama\"] | year_released: 2019 | popularity: 1.256 | vote_average: 6.3 | vote_count: 9.0 | runtime: 121.0 | production_countries: [\"Bulgaria\",\"Germany\"] | original_language: de | spoken_languages: [\"Magyar\",\"Deutsch\"] | overview: The horse trainer Wiebke adopts Raya from abroad, but soon she has to learn that the girl suffers from an attachment disorder and does not build an emotional connection to anybody around her. Raya constantly puts others in great danger, especially her older adoptive sister Nicolina. After a neurologist explains that Raya will have life-long troubles and does not feel any empathy, Wiebke has to decide whether she is willing to keep her and risk Nicolina’s well-being.\n",
      "\n",
      "Stringification V2\n",
      "Title: Pelican Blood (2019)\n",
      "Genres: drama\n",
      "Runtime: 2h 1m\n",
      "Average rating: 6.30 (9 vote(s))\n",
      "Production countries: Bulgaria and Germany\n",
      "Languages: Magyar and Deutsch\n",
      "Overview: The horse trainer Wiebke adopts Raya from abroad, but soon she has to learn that the girl suffers from an attachment disorder and does not build an emotional connection to anybody around her. Raya constantly puts others in great danger, especially her older adoptive sister Nicolina. After a neurologist explains that Raya will have life-long troubles and does not feel any empathy, Wiebke has to decide whether she is willing to keep her and risk Nicolina’s well-being.\n"
     ]
    }
   ],
   "source": [
    "print(\"Stringification V1\")\n",
    "print(format_movie_data(movie_data.iloc[100]))\n",
    "print()\n",
    "print(\"Stringification V2\")\n",
    "print(format_movie_data_v2(movie_data.iloc[100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# user samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating_val</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5fc57c5d6758f6963451a07f</td>\n",
       "      <td>feast-2014</td>\n",
       "      <td>7</td>\n",
       "      <td>deathproof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5fc57c5d6758f6963451a063</td>\n",
       "      <td>loving-2016</td>\n",
       "      <td>7</td>\n",
       "      <td>deathproof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5fc57c5d6758f6963451a0ef</td>\n",
       "      <td>scripted-content</td>\n",
       "      <td>7</td>\n",
       "      <td>deathproof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5fc57c5d6758f6963451a060</td>\n",
       "      <td>the-future</td>\n",
       "      <td>4</td>\n",
       "      <td>deathproof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5fc57c5c6758f69634519398</td>\n",
       "      <td>mank</td>\n",
       "      <td>5</td>\n",
       "      <td>deathproof</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id          movie_id  rating_val     user_id\n",
       "0  5fc57c5d6758f6963451a07f        feast-2014           7  deathproof\n",
       "1  5fc57c5d6758f6963451a063       loving-2016           7  deathproof\n",
       "2  5fc57c5d6758f6963451a0ef  scripted-content           7  deathproof\n",
       "3  5fc57c5d6758f6963451a060        the-future           4  deathproof\n",
       "4  5fc57c5c6758f69634519398              mank           5  deathproof"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv(\"../data/ratings_export.csv\")\n",
    "ratings.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning GPT2 model\n",
    "\n",
    "We will stream this dataset, because it takes a while to precompute it. We would rather like to see the language model's progress as it trains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import tqdm\n",
    "\n",
    "class MovieRatingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ratings, n_context_movies: int):\n",
    "        self.n_context_movies = n_context_movies\n",
    "        \n",
    "        user_counts = ratings.groupby('user_id').size().sort_values(ascending=False)\n",
    "        user_ids = user_counts[user_counts > 10].index\n",
    "\n",
    "        # Filter the ratings DataFrame to only include these users\n",
    "        ratings = ratings[ratings['user_id'].isin(user_ids)]\n",
    "        \n",
    "        # Filter to movie ids that are in the movie index\n",
    "        ratings = ratings[ratings['movie_id'].isin(movie_data.index)]\n",
    "        \n",
    "        # Generate rating spans\n",
    "        rating_spans = []\n",
    "        self.ratings_per_user = {}\n",
    "        \n",
    "        with tqdm.tqdm(user_ids, desc='Generating rating spans...') as pbar:\n",
    "            grouped = ratings.groupby('user_id')\n",
    "            for user_id in pbar:\n",
    "                user_ratings = grouped.get_group(user_id)\n",
    "                n_ratings = len(user_ratings)\n",
    "                for start_i in range(0, max(1, n_ratings - n_context_movies)):\n",
    "                    rating_spans.append((user_id, start_i, min(n_ratings, start_i + n_context_movies)))\n",
    "\n",
    "                self.ratings_per_user[user_id] = user_ratings\n",
    "                \n",
    "        self.rating_spans = rating_spans\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.rating_spans)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        (user_id, start_index, end_index) = self.rating_spans[index]\n",
    "        \n",
    "        ratings = self.ratings_per_user[user_id].iloc[start_index:end_index]\n",
    "        \n",
    "        # Construct target string.\n",
    "        target = \"\"\n",
    "        for i, rating in ratings.iterrows():\n",
    "            target += '---\\n' + format_movie_data_v2(movie_data.loc[rating['movie_id']]) + f\"\\n---\\nRating: {rating['rating_val']} / 10\\n\"\n",
    "            \n",
    "        return target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating rating spans...: 100%|██████████| 7366/7366 [00:09<00:00, 745.99it/s] \n"
     ]
    }
   ],
   "source": [
    "dataset = MovieRatingDataset(ratings, n_context_movies=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Title: The Concert (1962)\n",
      "Genres: music and animation\n",
      "Runtime: 0h 7m\n",
      "Average rating: 5.10 (20 vote(s))\n",
      "Production countries: France\n",
      "Languages: N/A\n",
      "Overview: A short film by Walerian Borowczyk. Ostensibly a film of a concert given by the round, unassuming Monsieur Kabal and his spiky, terrifying wife, it's actually a cover for their frequent attempts at causing each other extreme physical harm.\n",
      "---\n",
      "Rating: 3 / 10\n",
      "---\n",
      "Title: Regular Show: The Movie (2015)\n",
      "Genres: animation and comedy and science fiction and tv movie\n",
      "Runtime: 1h 10m\n",
      "Average rating: 7.80 (247 vote(s))\n",
      "Production countries: United States of America\n",
      "Languages: English\n",
      "Overview: To save the universe, and their friendship, Mordecai and Rigby must defeat an evil volleyball coach.\n",
      "---\n",
      "Rating: 4 / 10\n",
      "---\n",
      "Title: Yevade Subramanyam (2015)\n",
      "Genres: adventure and comedy and drama\n",
      "Runtime: 2h 40m\n",
      "Average rating: 7.40 (10 vote(s))\n",
      "Production countries: N/A\n",
      "Languages: తెలుగు\n",
      "Overview: A corporate man who sets out on a journey of self-discovery and heads to the Himalayas.\n",
      "---\n",
      "Rating: 3 / 10\n",
      "---\n",
      "Title: Singh Is Bliing (2015)\n",
      "Genres: action and comedy\n",
      "Runtime: 2h 20m\n",
      "Average rating: 5.80 (46 vote(s))\n",
      "Production countries: India\n",
      "Languages: ਪੰਜਾਬੀ and हिन्दी and English\n",
      "Overview: Raftaar Singh is always looking to have fun and runs away from responsibility. Fed up, his father orders Raftaar to go to Goa and work, and learn to take on responsibility. Once in Goa, he impresses his new boss with his enthusiasm and creative thinking. Soon he gets an assignment, and meets Sara Amy Jackson - and thus begins their unique love story. What ensues is a great comedy of errors, as one hilarious situation after another unfolds. Raftaar falls head over heels for this exotic, mysterious girl. The romance takes a deadly turn when he realizes that Sara is not who he thinks she is and has an agenda, which takes him to Romania. Will Raftaar be able to win her over in a distant and dangerous land?\n",
      "---\n",
      "Rating: 3 / 10\n",
      "---\n",
      "Title: The Little Prince (2015)\n",
      "Genres: adventure and animation and fantasy and family\n",
      "Runtime: 1h 48m\n",
      "Average rating: 7.60 (2254 vote(s))\n",
      "Production countries: France\n",
      "Languages: English\n",
      "Overview: Based on the best-seller book 'The Little Prince', the movie tells the story of a little girl that lives with resignation in a world where efficiency and work are the only dogmas. Everything will change when accidentally she discovers her neighbor that will tell her about the story of the Little Prince that he once met.\n",
      "---\n",
      "Rating: 7 / 10\n",
      "---\n",
      "Title: Ellis (2015)\n",
      "Genres: drama\n",
      "Runtime: 0h 14m\n",
      "Average rating: 6.70 (58 vote(s))\n",
      "Production countries: United States of America\n",
      "Languages: English\n",
      "Overview: Ellis, a fourteen-minute film directed by JR and written by Academy Award winner Eric Roth, tells the elusive story of countless immigrants whose pursuit of a new life led them to the now-shuttered Ellis Island Immigrant Hospital. Following its opening in 1902, approximately 1.2 million people passed through the facility, where the Statue of Liberty can be seen from the windows. Languishing in a sort of purgatory awaiting their fate, many were never discharged.\n",
      "---\n",
      "Rating: 4 / 10\n",
      "---\n",
      "Title: Huevos: Little Rooster's Egg-Cellent Adventure (2015)\n",
      "Genres: animation and family\n",
      "Runtime: 1h 39m\n",
      "Average rating: 7.10 (165 vote(s))\n",
      "Production countries: Mexico\n",
      "Languages: Español\n",
      "Overview: Toto has grown and now is a young cock. An evil rancher tricks the female owner of the ranch where they live and forces her to bet the property in a cockfight in the arena. Toto is the only option to defend their home, so he must train in just one week and win the championship of the arena.\n",
      "---\n",
      "Rating: 4 / 10\n",
      "---\n",
      "Title: Coasting Scene at Montmorency Falls, Canada (1902)\n",
      "Genres: \n",
      "Runtime: 0h 1m\n",
      "Average rating: 6.00 (1 vote(s))\n",
      "Production countries: N/A\n",
      "Languages: N/A\n",
      "Overview: A most amusing Winter scene. Shows a large number of boys, girls, men and women on top of an incline with their sleds and toboggans. They coast down the incline at a very high rate of speed.\n",
      "---\n",
      "Rating: 1 / 10\n",
      "---\n",
      "Title: Heist (2015)\n",
      "Genres: action\n",
      "Runtime: 1h 33m\n",
      "Average rating: 6.10 (736 vote(s))\n",
      "Production countries: United States of America\n",
      "Languages: English\n",
      "Overview: A father is without the means to pay for his daughter's medical treatment. As a last resort, he partners with a greedy co-worker to rob a casino. When things go awry they're forced to hijack a city bus.\n",
      "---\n",
      "Rating: 5 / 10\n",
      "---\n",
      "Title: My Expectations in 1908 (1908)\n",
      "Genres: \n",
      "Runtime: 0h 4m\n",
      "Average rating: 5.00 (1 vote(s))\n",
      "Production countries: N/A\n",
      "Languages: N/A\n",
      "Overview: We can see the children of the director playing and dancing, obviously getting told what to do while it happens, and later on his wife knitting with the children around her.\n",
      "---\n",
      "Rating: 2 / 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = '/scratch/gsk6me/huggingface_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt2 = transformers.GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\").to(\"cuda\")\n",
    "gpt2_tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/Indexing.cu:975: indexSelectLargeIndex: block: [148,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train to maximize the ll of this string.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m tokenization \u001b[38;5;241m=\u001b[39m gpt2_tokenizer(string, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mgpt2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenization\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/sfs/weka/scratch/gsk6me/WORLDMODELS/rvt/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/sfs/weka/scratch/gsk6me/WORLDMODELS/rvt/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/sfs/weka/scratch/gsk6me/WORLDMODELS/rvt/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/sfs/weka/scratch/gsk6me/WORLDMODELS/rvt/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/sfs/weka/scratch/gsk6me/WORLDMODELS/rvt/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/sfs/weka/scratch/gsk6me/WORLDMODELS/rvt/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/sfs/weka/scratch/gsk6me/WORLDMODELS/rvt/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/sfs/weka/scratch/gsk6me/WORLDMODELS/rvt/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:312\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    310\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m encoder_attention_mask\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     query, key, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_size, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    314\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_heads(query, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    315\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_heads(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n",
      "File \u001b[0;32m/sfs/weka/scratch/gsk6me/WORLDMODELS/rvt/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/sfs/weka/scratch/gsk6me/WORLDMODELS/rvt/lib/python3.8/site-packages/transformers/pytorch_utils.py:103\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    102\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[0;32m--> 103\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "optimizer = torch.optim.Adam(gpt2.parameters(), lr=1e-4)\n",
    "\n",
    "order = torch.randperm(len(dataset))\n",
    "for step in range(100000):\n",
    "    index = order[step]\n",
    "    string = dataset[index]\n",
    "    \n",
    "    # Train to maximize the ll of this string.\n",
    "    tokenization = gpt2_tokenizer(string, return_tensors='pt')['input_ids'].cuda()\n",
    "    output = gpt2(tokenization)\n",
    "    print(output)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def construct_movie_string_dictionary():\n",
    "    results = {}\n",
    "    with tqdm.tqdm(list(movie_data.index), desc='Serializing movie data...') as pbar:\n",
    "        for movie_id in pbar:\n",
    "            results[movie_id] = format_movie_data_v2(movie_data.loc[movie_id])\n",
    "    return results\n",
    "\n",
    "movie_string_dictionary = construct_movie_string_dictionary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct some strings for each of these users\n",
    "def construct_ratings_set():\n",
    "    user_ids = []\n",
    "    rating_strings = []\n",
    "\n",
    "    for user_id in tqdm.tqdm(users_filtered, desc='Creating dataset...'):\n",
    "        user_ratings = filtered_ratings[filtered_ratings['user_id'] == user_id]\n",
    "\n",
    "        # construct strings from each of these users\n",
    "        user_rating_strings = []\n",
    "        for rating_id, (_id, movie_id, rating_val, _user_id) in user_ratings.iterrows():\n",
    "            if movie_id not in movie_data.index:\n",
    "                continue\n",
    "            movie_data = movie_data.loc[movie_id]\n",
    "            user_rating_strings.append(format_movie_data_v2(movie_data, rating_val, \"N/A\"))\n",
    "        user_ids.append(user_id)\n",
    "        rating_strings.append(user_rating_strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_ids = filtered_ratings['user_id'].unique() \n",
    "random.shuffle(all_user_ids)\n",
    "train_cutoff = int(len(all_user_ids)*4/5)\n",
    "train_movie_ids = all_user_ids[:train_cutoff]\n",
    "test_movie_ids = all_user_ids[train_cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rtown004'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_movie_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_samples = []\n",
    "system_prompt = \"A movie recommendation system which takes in a list of movies and outputs the title of the next movie to watch. Only output the title of the movie.\"\n",
    "user_history = \"I have watched the following movies:\\n\"\n",
    "\n",
    "for user in train_movie_ids[:1]:\n",
    "    rating_history = filtered_ratings[filtered_ratings['user_id'] == user]\n",
    "    combined = rating_history.merge(movie_data, on='movie_id')\n",
    "    combined['movie_string'] = combined.apply(format_movie_data, axis=1)\n",
    "\n",
    "    # split data into groups of 11 where 10 are the user's history and the last is the next movie to watch\n",
    "    for i in range(0, len(combined), 10):\n",
    "        if i + 10 >= len(combined):\n",
    "            break\n",
    "        watch_history = user_history + \"\\n\".join(combined['movie_string'][i:i+10])\n",
    "        next_movie = combined['movie_title'][i+10]\n",
    "        sample = {\n",
    "            \"messages\": \n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": f\"{system_prompt}\"}, \n",
    "                    {\"role\": \"user\", \"content\": f\"{watch_history}\"}, \n",
    "                    {\"role\": \"assistant\", \"content\": f\"{next_movie}\"}\n",
    "                ]\n",
    "        }\n",
    "        training_samples.append(sample)\n",
    "\n",
    "with open('training_samples.jsonl', 'w') as file:\n",
    "    for sample in training_samples:\n",
    "        json_line = json.dumps(sample)\n",
    "        file.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "job_id = \"\"\n",
    "client.fine_tuning.jobs.retrieve(f\"{job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = OpenAI()\n",
    "# completion = client.chat.completions.create(\n",
    "#   model=\"ft:gpt-3.5-turbo:my-org:custom_suffix:id\",\n",
    "#   messages=[\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "#   ]\n",
    "# )\n",
    "# print(completion.choices[0].message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rvt",
   "language": "python",
   "name": "rvt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
